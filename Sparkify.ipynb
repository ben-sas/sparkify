{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Project Workspace\n",
    "This workspace contains a tiny subset (128MB) of the full dataset available (12GB). Feel free to use this workspace to build your project, or to explore a smaller subset with Spark before deploying your cluster on the cloud. Instructions for setting up your Spark cluster is included in the last lesson of the Extracurricular Spark Course content.\n",
    "\n",
    "You can follow the steps below to guide your data analysis and model building portion of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import asc\n",
    "from pyspark.sql.functions import sum as Fsum, isnan, to_date, to_timestamp, from_unixtime\n",
    "\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Wrangling Data\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean Dataset\n",
    "In this workspace, the mini-dataset file is `mini_sparkify_event_data.json`. Load and clean the dataset, checking for invalid or missing data - for example, records without userids or sessionids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"mini_sparkify_event_data.json\"\n",
    "df = spark.read.json(path)\n",
    "df_original = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>auth</th>\n",
       "      <th>firstName</th>\n",
       "      <th>gender</th>\n",
       "      <th>itemInSession</th>\n",
       "      <th>lastName</th>\n",
       "      <th>length</th>\n",
       "      <th>level</th>\n",
       "      <th>location</th>\n",
       "      <th>method</th>\n",
       "      <th>page</th>\n",
       "      <th>registration</th>\n",
       "      <th>sessionId</th>\n",
       "      <th>song</th>\n",
       "      <th>status</th>\n",
       "      <th>ts</th>\n",
       "      <th>userAgent</th>\n",
       "      <th>userId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Martha Tilston</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Colin</td>\n",
       "      <td>M</td>\n",
       "      <td>50</td>\n",
       "      <td>Freeman</td>\n",
       "      <td>277.89016</td>\n",
       "      <td>paid</td>\n",
       "      <td>Bakersfield, CA</td>\n",
       "      <td>PUT</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>1538173362000</td>\n",
       "      <td>29</td>\n",
       "      <td>Rockpools</td>\n",
       "      <td>200</td>\n",
       "      <td>1538352117000</td>\n",
       "      <td>Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) G...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Five Iron Frenzy</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Micah</td>\n",
       "      <td>M</td>\n",
       "      <td>79</td>\n",
       "      <td>Long</td>\n",
       "      <td>236.09424</td>\n",
       "      <td>free</td>\n",
       "      <td>Boston-Cambridge-Newton, MA-NH</td>\n",
       "      <td>PUT</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>1538331630000</td>\n",
       "      <td>8</td>\n",
       "      <td>Canada</td>\n",
       "      <td>200</td>\n",
       "      <td>1538352180000</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adam Lambert</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Colin</td>\n",
       "      <td>M</td>\n",
       "      <td>51</td>\n",
       "      <td>Freeman</td>\n",
       "      <td>282.82730</td>\n",
       "      <td>paid</td>\n",
       "      <td>Bakersfield, CA</td>\n",
       "      <td>PUT</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>1538173362000</td>\n",
       "      <td>29</td>\n",
       "      <td>Time For Miracles</td>\n",
       "      <td>200</td>\n",
       "      <td>1538352394000</td>\n",
       "      <td>Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) G...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Enigma</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Micah</td>\n",
       "      <td>M</td>\n",
       "      <td>80</td>\n",
       "      <td>Long</td>\n",
       "      <td>262.71302</td>\n",
       "      <td>free</td>\n",
       "      <td>Boston-Cambridge-Newton, MA-NH</td>\n",
       "      <td>PUT</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>1538331630000</td>\n",
       "      <td>8</td>\n",
       "      <td>Knocking On Forbidden Doors</td>\n",
       "      <td>200</td>\n",
       "      <td>1538352416000</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Daft Punk</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Colin</td>\n",
       "      <td>M</td>\n",
       "      <td>52</td>\n",
       "      <td>Freeman</td>\n",
       "      <td>223.60771</td>\n",
       "      <td>paid</td>\n",
       "      <td>Bakersfield, CA</td>\n",
       "      <td>PUT</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>1538173362000</td>\n",
       "      <td>29</td>\n",
       "      <td>Harder Better Faster Stronger</td>\n",
       "      <td>200</td>\n",
       "      <td>1538352676000</td>\n",
       "      <td>Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) G...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             artist       auth firstName gender  itemInSession lastName  \\\n",
       "0    Martha Tilston  Logged In     Colin      M             50  Freeman   \n",
       "1  Five Iron Frenzy  Logged In     Micah      M             79     Long   \n",
       "2      Adam Lambert  Logged In     Colin      M             51  Freeman   \n",
       "3            Enigma  Logged In     Micah      M             80     Long   \n",
       "4         Daft Punk  Logged In     Colin      M             52  Freeman   \n",
       "\n",
       "      length level                        location method      page  \\\n",
       "0  277.89016  paid                 Bakersfield, CA    PUT  NextSong   \n",
       "1  236.09424  free  Boston-Cambridge-Newton, MA-NH    PUT  NextSong   \n",
       "2  282.82730  paid                 Bakersfield, CA    PUT  NextSong   \n",
       "3  262.71302  free  Boston-Cambridge-Newton, MA-NH    PUT  NextSong   \n",
       "4  223.60771  paid                 Bakersfield, CA    PUT  NextSong   \n",
       "\n",
       "    registration  sessionId                           song  status  \\\n",
       "0  1538173362000         29                      Rockpools     200   \n",
       "1  1538331630000          8                         Canada     200   \n",
       "2  1538173362000         29              Time For Miracles     200   \n",
       "3  1538331630000          8    Knocking On Forbidden Doors     200   \n",
       "4  1538173362000         29  Harder Better Faster Stronger     200   \n",
       "\n",
       "              ts                                          userAgent userId  \n",
       "0  1538352117000  Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) G...     30  \n",
       "1  1538352180000  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...      9  \n",
       "2  1538352394000  Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) G...     30  \n",
       "3  1538352416000  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...      9  \n",
       "4  1538352676000  Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) G...     30  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Easier visual exploration as Pandas data frame\n",
    "df.limit(5).toPandas()\n",
    "\n",
    "# Questions to explore:\n",
    "# sessionId unique across users? (can two users have the same sessionId?)\n",
    "# what is the column \"method\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                page|\n",
      "+--------------------+\n",
      "|               About|\n",
      "|          Add Friend|\n",
      "|     Add to Playlist|\n",
      "|              Cancel|\n",
      "|Cancellation Conf...|\n",
      "|           Downgrade|\n",
      "|               Error|\n",
      "|                Help|\n",
      "|                Home|\n",
      "|               Login|\n",
      "|              Logout|\n",
      "|            NextSong|\n",
      "|            Register|\n",
      "|         Roll Advert|\n",
      "|       Save Settings|\n",
      "|            Settings|\n",
      "|    Submit Downgrade|\n",
      "| Submit Registration|\n",
      "|      Submit Upgrade|\n",
      "|         Thumbs Down|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------+\n",
      "|      auth|\n",
      "+----------+\n",
      "| Cancelled|\n",
      "|     Guest|\n",
      "| Logged In|\n",
      "|Logged Out|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Which pages do users interact with?\n",
    "df.select(\"page\").dropDuplicates().sort(\"page\").show()\n",
    "\n",
    "df.select(\"auth\").dropDuplicates().sort(\"auth\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/52319958/pyspark-select-the-distinct-values-from-each-column\n",
    "import pyspark.sql.functions as f\n",
    "# df.select(*[f.collect_set(c).alias(c) for c in df.columns]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>auth</th>\n",
       "      <th>firstName</th>\n",
       "      <th>gender</th>\n",
       "      <th>itemInSession</th>\n",
       "      <th>lastName</th>\n",
       "      <th>length</th>\n",
       "      <th>level</th>\n",
       "      <th>location</th>\n",
       "      <th>method</th>\n",
       "      <th>page</th>\n",
       "      <th>registration</th>\n",
       "      <th>sessionId</th>\n",
       "      <th>song</th>\n",
       "      <th>status</th>\n",
       "      <th>ts</th>\n",
       "      <th>userAgent</th>\n",
       "      <th>userId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17655</td>\n",
       "      <td>4</td>\n",
       "      <td>189</td>\n",
       "      <td>2</td>\n",
       "      <td>1322</td>\n",
       "      <td>173</td>\n",
       "      <td>14865</td>\n",
       "      <td>2</td>\n",
       "      <td>114</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>225</td>\n",
       "      <td>2354</td>\n",
       "      <td>58480</td>\n",
       "      <td>3</td>\n",
       "      <td>277447</td>\n",
       "      <td>56</td>\n",
       "      <td>226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   artist  auth  firstName  gender  itemInSession  lastName  length  level  \\\n",
       "0   17655     4        189       2           1322       173   14865      2   \n",
       "\n",
       "   location  method  page  registration  sessionId   song  status      ts  \\\n",
       "0       114       2    22           225       2354  58480       3  277447   \n",
       "\n",
       "   userAgent  userId  \n",
       "0         56     226  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distinct values per column\n",
    "# https://stackoverflow.com/questions/40888946/spark-dataframe-count-distinct-values-of-every-column\n",
    "from pyspark.sql.functions import col, countDistinct\n",
    "\n",
    "distinct_value_count = df.agg(*(countDistinct(col(c)).alias(c) for c in df.columns)).toPandas()\n",
    "distinct_value_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>auth</th>\n",
       "      <th>firstName</th>\n",
       "      <th>gender</th>\n",
       "      <th>itemInSession</th>\n",
       "      <th>lastName</th>\n",
       "      <th>length</th>\n",
       "      <th>level</th>\n",
       "      <th>location</th>\n",
       "      <th>method</th>\n",
       "      <th>page</th>\n",
       "      <th>registration</th>\n",
       "      <th>sessionId</th>\n",
       "      <th>song</th>\n",
       "      <th>status</th>\n",
       "      <th>ts</th>\n",
       "      <th>userAgent</th>\n",
       "      <th>userId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   artist  auth  firstName  gender  itemInSession  lastName  length  level  \\\n",
       "0    True  True       True   False           True      True    True  False   \n",
       "\n",
       "   location  method  page  registration  sessionId  song  status    ts  \\\n",
       "0      True   False  True          True       True  True    True  True   \n",
       "\n",
       "   userAgent  userId  \n",
       "0       True    True  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinct_value_count > 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_hour = udf(lambda x: datetime.datetime.fromtimestamp(x / 1000.0). hour)\n",
    "# get_date = udf(lambda x: datetime.datetime.fromtimestamp(x / 1000.0). date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(artist='Martha Tilston', auth='Logged In', firstName='Colin', gender='M', itemInSession=50, lastName='Freeman', length=277.89016, level='paid', location='Bakersfield, CA', method='PUT', page='NextSong', registration=1538173362000, sessionId=29, song='Rockpools', status=200, ts=1538352117000, userAgent='Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0', userId='30')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/48229043/python-pyspark-count-null-empty-and-nan\n",
    "df.filter((df[\"ts\"] == \"\") | df[\"ts\"].isNull() | isnan(df[\"ts\"])).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import LongType, TimestampType\n",
    "from pyspark.sql.functions import substring, from_unixtime\n",
    "\n",
    "df = df_original\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      " |-- ts_new: string (nullable = true)\n",
      " |-- ts_new_2: timestamp (nullable = true)\n",
      " |-- ts_unix: string (nullable = true)\n",
      " |-- ts_unix_trunc: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(artist='Martha Tilston', auth='Logged In', firstName='Colin', gender='M', itemInSession=50, lastName='Freeman', length=277.89016, level='paid', location='Bakersfield, CA', method='PUT', page='NextSong', registration=1538173362000, sessionId=29, song='Rockpools', status=200, ts=1538352117000, userAgent='Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0', userId='30', ts_new='1538352117', ts_new_2=None, ts_unix='+50718-06-10 10:30:00', ts_unix_trunc='2018-10-01 02:01:57')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.withColumn(\"ts_new\", substring(df.ts, 0, 10))\n",
    "df = df.withColumn(\"ts_new_2\", df.ts_new.cast(TimestampType()))\n",
    "\n",
    "df = df.withColumn(\"ts_unix\", from_unixtime(df.ts))\n",
    "df = df.withColumn(\"ts_unix_trunc\", from_unixtime(df.ts_new))\n",
    "\n",
    "\n",
    "df.printSchema()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ts=1543799476000),\n",
       " Row(ts=1543622579000),\n",
       " Row(ts=1543622432000),\n",
       " Row(ts=1543622411000),\n",
       " Row(ts=1543622398000),\n",
       " Row(ts=1543622395000),\n",
       " Row(ts=1543622365000),\n",
       " Row(ts=1543622355000),\n",
       " Row(ts=1543622320000),\n",
       " Row(ts=1543622287000)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"ts\").dropDuplicates().sort(\"ts\", ascending=False).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"hour\", get_hour(df.ts))\n",
    "# df = df.withColumn(\"date\", to_date(\"ts\"))\n",
    "df = df.withColumn(\"ts_new\", to_timestamp(\"ts\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"c:\\Users\\q514907\\Anaconda3\\envs\\spark_env\\lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1247, in main\n  File \"c:\\Users\\q514907\\Anaconda3\\envs\\spark_env\\lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1239, in process\n  File \"c:\\Users\\q514907\\Anaconda3\\envs\\spark_env\\lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 225, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"c:\\Users\\q514907\\Anaconda3\\envs\\spark_env\\lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 146, in dump_stream\n    for obj in iterator:\n  File \"c:\\Users\\q514907\\Anaconda3\\envs\\spark_env\\lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 214, in _batched\n    for item in iterator:\n  File \"c:\\Users\\q514907\\Anaconda3\\envs\\spark_env\\lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1070, in mapper\n  File \"c:\\Users\\q514907\\Anaconda3\\envs\\spark_env\\lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1070, in <genexpr>\n  File \"c:\\Users\\q514907\\Anaconda3\\envs\\spark_env\\lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 104, in <lambda>\n  File \"c:\\Users\\q514907\\Anaconda3\\envs\\spark_env\\lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\sql\\types.py\", line 275, in toInternal\n    calendar.timegm(dt.utctimetuple()) if dt.tzinfo else time.mktime(dt.timetuple())\nAttributeError: 'float' object has no attribute 'tzinfo'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32mc:\\develop\\VSCode\\Udacity\\capstone_sparkify\\Sparkify.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/develop/VSCode/Udacity/capstone_sparkify/Sparkify.ipynb#X42sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# get_date = udf(lambda x: (x / 1000.0), returnType=LongType())\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/develop/VSCode/Udacity/capstone_sparkify/Sparkify.ipynb#X42sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/develop/VSCode/Udacity/capstone_sparkify/Sparkify.ipynb#X42sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/develop/VSCode/Udacity/capstone_sparkify/Sparkify.ipynb#X42sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# df = df.withColumn(\"ts_new\", df.ts)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/develop/VSCode/Udacity/capstone_sparkify/Sparkify.ipynb#X42sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mwithColumn(\u001b[39m\"\u001b[39m\u001b[39mts_new\u001b[39m\u001b[39m\"\u001b[39m, get_date(df\u001b[39m.\u001b[39mts))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/develop/VSCode/Udacity/capstone_sparkify/Sparkify.ipynb#X42sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m df\u001b[39m.\u001b[39;49mhead()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/develop/VSCode/Udacity/capstone_sparkify/Sparkify.ipynb#X42sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m df\u001b[39m.\u001b[39mselect([\u001b[39m\"\u001b[39m\u001b[39mts\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mts_new\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mhead()\n",
      "File \u001b[1;32mc:\\Users\\q514907\\Anaconda3\\envs\\spark_env\\lib\\site-packages\\pyspark\\sql\\dataframe.py:2967\u001b[0m, in \u001b[0;36mDataFrame.head\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   2935\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns the first ``n`` rows.\u001b[39;00m\n\u001b[0;32m   2936\u001b[0m \n\u001b[0;32m   2937\u001b[0m \u001b[39m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2964\u001b[0m \u001b[39m[Row(age=2, name='Alice')]\u001b[39;00m\n\u001b[0;32m   2965\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2966\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 2967\u001b[0m     rs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhead(\u001b[39m1\u001b[39;49m)\n\u001b[0;32m   2968\u001b[0m     \u001b[39mreturn\u001b[39;00m rs[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m rs \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   2969\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(n)\n",
      "File \u001b[1;32mc:\\Users\\q514907\\Anaconda3\\envs\\spark_env\\lib\\site-packages\\pyspark\\sql\\dataframe.py:2969\u001b[0m, in \u001b[0;36mDataFrame.head\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   2967\u001b[0m     rs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead(\u001b[39m1\u001b[39m)\n\u001b[0;32m   2968\u001b[0m     \u001b[39mreturn\u001b[39;00m rs[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m rs \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 2969\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtake(n)\n",
      "File \u001b[1;32mc:\\Users\\q514907\\Anaconda3\\envs\\spark_env\\lib\\site-packages\\pyspark\\sql\\dataframe.py:1401\u001b[0m, in \u001b[0;36mDataFrame.take\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1372\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtake\u001b[39m(\u001b[39mself\u001b[39m, num: \u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Row]:\n\u001b[0;32m   1373\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Returns the first ``num`` rows as a :class:`list` of :class:`Row`.\u001b[39;00m\n\u001b[0;32m   1374\u001b[0m \n\u001b[0;32m   1375\u001b[0m \u001b[39m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1399\u001b[0m \u001b[39m    [Row(age=14, name='Tom'), Row(age=23, name='Alice')]\u001b[39;00m\n\u001b[0;32m   1400\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1401\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlimit(num)\u001b[39m.\u001b[39;49mcollect()\n",
      "File \u001b[1;32mc:\\Users\\q514907\\Anaconda3\\envs\\spark_env\\lib\\site-packages\\pyspark\\sql\\dataframe.py:1257\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1237\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[0;32m   1238\u001b[0m \n\u001b[0;32m   1239\u001b[0m \u001b[39m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1254\u001b[0m \u001b[39m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[0;32m   1255\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1256\u001b[0m \u001b[39mwith\u001b[39;00m SCCallSiteSync(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sc):\n\u001b[1;32m-> 1257\u001b[0m     sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mcollectToPython()\n\u001b[0;32m   1258\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[1;32mc:\\Users\\q514907\\Anaconda3\\envs\\spark_env\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\q514907\\Anaconda3\\envs\\spark_env\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    186\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"c:\\Users\\q514907\\Anaconda3\\envs\\spark_env\\lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1247, in main\n  File \"c:\\Users\\q514907\\Anaconda3\\envs\\spark_env\\lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1239, in process\n  File \"c:\\Users\\q514907\\Anaconda3\\envs\\spark_env\\lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 225, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"c:\\Users\\q514907\\Anaconda3\\envs\\spark_env\\lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 146, in dump_stream\n    for obj in iterator:\n  File \"c:\\Users\\q514907\\Anaconda3\\envs\\spark_env\\lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 214, in _batched\n    for item in iterator:\n  File \"c:\\Users\\q514907\\Anaconda3\\envs\\spark_env\\lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1070, in mapper\n  File \"c:\\Users\\q514907\\Anaconda3\\envs\\spark_env\\lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1070, in <genexpr>\n  File \"c:\\Users\\q514907\\Anaconda3\\envs\\spark_env\\lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 104, in <lambda>\n  File \"c:\\Users\\q514907\\Anaconda3\\envs\\spark_env\\lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\sql\\types.py\", line 275, in toInternal\n    calendar.timegm(dt.utctimetuple()) if dt.tzinfo else time.mktime(dt.timetuple())\nAttributeError: 'float' object has no attribute 'tzinfo'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# df = df.withColumn(\"ts_new\", to_timestamp(df[\"ts\"]))\n",
    "\n",
    "# df = df.withColumn(\"ts_new\", from_unixtime(\"ts\"))\n",
    "# get_date = udf(lambda x: from_unixtime(x / 1000.0))\n",
    "get_date = udf(lambda x: (x / 1000.0), TimestampType())\n",
    "# get_date = udf(lambda x: (x / 1000.0), returnType=LongType())\n",
    "\n",
    "\n",
    "# df = df.withColumn(\"ts_new\", df.ts)\n",
    "df = df.withColumn(\"ts_new\", get_date(df.ts))\n",
    "\n",
    "df.head()\n",
    "df.select([\"ts\", \"ts_new\"]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(artist='Martha Tilston', auth='Logged In', firstName='Colin', gender='M', itemInSession=50, lastName='Freeman', length=277.89016, level='paid', location='Bakersfield, CA', method='PUT', page='NextSong', registration=1538173362000, sessionId=29, song='Rockpools', status=200, ts=1538352117000, userAgent='Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0', userId='30', ts_new=1538352117000)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n",
    "\n",
    "# df.select(\"date\").dropDuplicates().sort(\"date\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "When you're working with the full dataset, perform EDA by loading a small subset of the data and doing basic manipulations within Spark. In this workspace, you are already provided a small subset of data you can explore.\n",
    "\n",
    "### Define Churn\n",
    "\n",
    "Once you've done some preliminary analysis, create a column `Churn` to use as the label for your model. I suggest using the `Cancellation Confirmation` events to define your churn, which happen for both paid and free users. As a bonus task, you can also look into the `Downgrade` events.\n",
    "\n",
    "### Explore Data\n",
    "Once you've defined churn, perform some exploratory data analysis to observe the behavior for users who stayed vs users who churned. You can start by exploring aggregates on these two groups of users, observing how much of a specific action they experienced per a certain time unit or number of songs played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Once you've familiarized yourself with the data, build out the features you find promising to train your model on. To work with the full dataset, you can follow the following steps.\n",
    "- Write a script to extract the necessary features from the smaller subset of data\n",
    "- Ensure that your script is scalable, using the best practices discussed in Lesson 3\n",
    "- Try your script on the full data set, debugging your script if necessary\n",
    "\n",
    "If you are working in the classroom workspace, you can just extract features based on the small subset of data contained here. Be sure to transfer over this work to the larger dataset when you work on your Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "Split the full dataset into train, test, and validation sets. Test out several of the machine learning methods you learned. Evaluate the accuracy of the various models, tuning parameters as necessary. Determine your winning model based on test accuracy and report results on the validation set. Since the churned users are a fairly small subset, I suggest using F1 score as the metric to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Steps\n",
    "Clean up your code, adding comments and renaming variables to make the code easier to read and maintain. Refer to the Spark Project Overview page and Data Scientist Capstone Project Rubric to make sure you are including all components of the capstone project and meet all expectations. Remember, this includes thorough documentation in a README file in a Github repository, as well as a web app or blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
